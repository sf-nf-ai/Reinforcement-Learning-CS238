{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4d9cSTY2_g2"
      },
      "outputs": [],
      "source": [
        "# For Google Colab\n",
        "!pip install gymnasium\n",
        "!pip install \"autorom[accept-rom-license]\"\n",
        "!pip install \"gymnasium[atari]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3K2f5kn3KP-"
      },
      "outputs": [],
      "source": [
        "# Connect to Google drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "# Load necessary packages\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "from IPython import display as ipythondisplay\n",
        "import pickle\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import seaborn as sns\n",
        "\n",
        "from datetime import date\n",
        "import os\n",
        "os.chdir('/content/path')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-RIi3vr3fgn"
      },
      "source": [
        "## Q-Learning Agent class\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NcI8ngto3K5X"
      },
      "outputs": [],
      "source": [
        "def default_q_values():\n",
        "  return np.zeros(env.action_space.n)\n",
        "\n",
        "class QLearningAgent:\n",
        "    \"\"\"\n",
        "    A Q-Learning agent with epsilon greedy strategy.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        learning_rate: float,\n",
        "        initial_epsilon: float,\n",
        "        epsilon_decay: float,\n",
        "        final_epsilon: float,\n",
        "        discount_factor: float = 0.95,\n",
        "    ):\n",
        "        \"\"\"Initialize a Reinforcement Learning agent with an empty dictionary\n",
        "        of state-action values (q_values), a learning rate and an epsilon.\n",
        "\n",
        "        Args:\n",
        "            learning_rate: The learning rate\n",
        "            initial_epsilon: The initial epsilon value\n",
        "            epsilon_decay: The decay for epsilon\n",
        "            final_epsilon: The final epsilon value\n",
        "            discount_factor: The discount factor for computing the Q-value\n",
        "        \"\"\"\n",
        "        self.q_values = defaultdict(default_q_values)\n",
        "\n",
        "        self.lr = learning_rate\n",
        "        self.discount_factor = discount_factor\n",
        "\n",
        "        self.epsilon = initial_epsilon\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.final_epsilon = final_epsilon\n",
        "\n",
        "        self.training_error = []\n",
        "\n",
        "    def get_action(self, obs: tuple) -> int:\n",
        "        \"\"\"\n",
        "        Returns the best action with probability (1 - epsilon)\n",
        "        otherwise a random action with probability epsilon to ensure exploration.\n",
        "        \"\"\"\n",
        "        # with probability epsilon return a random action to explore the environment\n",
        "        if np.random.random() < self.epsilon:\n",
        "            return env.action_space.sample()\n",
        "\n",
        "        # with probability (1 - epsilon) act greedily (exploit)\n",
        "        else:\n",
        "            return int(np.argmax(self.q_values[obs]))\n",
        "\n",
        "    def update(\n",
        "        self,\n",
        "        obs: tuple,\n",
        "        action: int,\n",
        "        reward: float,\n",
        "        terminated: bool,\n",
        "        next_obs: tuple,\n",
        "    ):\n",
        "        \"\"\"Updates the Q-value of an action.\"\"\"\n",
        "        future_q_value = (not terminated) * np.max(self.q_values[next_obs])\n",
        "        temporal_difference = (\n",
        "            reward + self.discount_factor * future_q_value - self.q_values[obs][action]\n",
        "        )\n",
        "\n",
        "        self.q_values[obs][action] = (\n",
        "            self.q_values[obs][action] + self.lr * temporal_difference\n",
        "        )\n",
        "        self.training_error.append(temporal_difference)\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        self.epsilon = max(self.final_epsilon, self.epsilon - epsilon_decay)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqqsM-WZ3_85"
      },
      "source": [
        "## Encoded State Space"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YzeAbTjSzzkY"
      },
      "outputs": [],
      "source": [
        "# Define constants for bricks, paddle, and ball\n",
        "\n",
        "PADDLE_WIDTH = 16\n",
        "PADDLE_HEIGHT = 4\n",
        "MARGIN_WIDTH = 8 # grey margin\n",
        "\n",
        "BALL_WIDTH = 2\n",
        "BALL_HEIGHT = 4\n",
        "BELOW_BRICK = 93\n",
        "ABOVE_PADDLE = 188\n",
        "BALL_COLOR_R = 200 # color of the ball in red channel\n",
        "\n",
        "num_bricks = 18\n",
        "brick_width = 8\n",
        "brick_height = 6\n",
        "start_pixel = 57 # first pixel of top brick row\n",
        "end_pixel = start_pixel + 5*brick_height + 1 # last top-left corner vertically\n",
        "margin_width = 8 # grey margin\n",
        "image_width = 160 - 2 * margin_width # 144\n",
        "\n",
        "X_INDICES = []\n",
        "Y_INDICES = []\n",
        "\n",
        "for row_pixel in range(start_pixel, end_pixel, brick_height):\n",
        "  for col_pixel in range(margin_width, image_width + margin_width - brick_width + 1, brick_width):\n",
        "    X_INDICES.append(col_pixel)\n",
        "    Y_INDICES.append(row_pixel)\n",
        "\n",
        "\n",
        "\n",
        "# Functions to extract state from image.\n",
        "\n",
        "# returns a 108-vector with zeros and ones indicating which bricks are broken (=0).\n",
        "def get_bricks(observation):\n",
        "  r = observation[:, :, 0]\n",
        "  brick_wall = r[Y_INDICES, X_INDICES] != 0\n",
        "  return brick_wall\n",
        "\n",
        "\n",
        "# get_paddle_loc() returns an integer that is the horizontal location of a left-most pixel of the paddle\n",
        "def get_paddle_loc(observation):\n",
        "  paddle_line = observation[190, :, 0]\n",
        "  middle_area = paddle_line[MARGIN_WIDTH:-MARGIN_WIDTH]\n",
        "  paddle_loc_indices = np.where(middle_area != 0)[0]\n",
        "  start_index = paddle_loc_indices[0]\n",
        "  end_index = paddle_loc_indices[-1]\n",
        "\n",
        "  # Paddle is inside left margin\n",
        "  if start_index == 0 and end_index < PADDLE_WIDTH:\n",
        "    paddle_left = MARGIN_WIDTH - (PADDLE_WIDTH - end_index)\n",
        "\n",
        "  # Paddle is inside right margin\n",
        "  elif end_index == len(middle_area) and (end_index - start_index) < PADDLE_WIDTH:\n",
        "    paddle_left = MARGIN_WIDTH + start_index\n",
        "\n",
        "  # Paddle is somewhere in the middle\n",
        "  else:\n",
        "    paddle_left = start_index + MARGIN_WIDTH\n",
        "\n",
        "  return np.array([paddle_left])\n",
        "\n",
        "\n",
        "# Returns an array with the coordinates of the top-left pixel of the ball.\n",
        "def get_ball_loc(observation, paddle_left):\n",
        "  middle_r = observation[BELOW_BRICK:ABOVE_PADDLE, :, 0] # image of the red channel\n",
        "  ball_indices = np.where(middle_r == BALL_COLOR_R)\n",
        "\n",
        "  if len(ball_indices[0]) != 0:\n",
        "    return np.array([ball_indices[0][0], ball_indices[1][0]])\n",
        "  else:\n",
        "    # If the ball does not in middle area, return unique value.\n",
        "    return np.array([-1,-1])\n",
        "\n",
        "\n",
        "# Concatenates all pieces of observations\n",
        "def get_states(observation):\n",
        "  bricks = get_bricks(observation)\n",
        "  paddle_left = get_paddle_loc(observation)\n",
        "  ball_loc = get_ball_loc(observation, paddle_left)\n",
        "\n",
        "  return tuple(np.concatenate((bricks, ball_loc, paddle_left)))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stage 1 training (decaying epsilon by 0.1 every 30_000 episodes)."
      ],
      "metadata": {
        "id": "cAFWe7m-s0ZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stem_path = '/content/agent_saves'\n",
        "\n",
        "# Hyperparameters\n",
        "max_epochs = 10\n",
        "learning_rate = 0.01\n",
        "n_episodes = 30_000\n",
        "start_epsilon = 1.0\n",
        "epsilon_decay = 0.1\n",
        "final_epsilon = 0.1\n",
        "\n",
        "today = date.today()\n",
        "\n",
        "# Q-Learning Agent\n",
        "MAP_agent = QLearningAgent(\n",
        "    learning_rate=learning_rate,\n",
        "    initial_epsilon=start_epsilon,\n",
        "    epsilon_decay=epsilon_decay,\n",
        "    final_epsilon=final_epsilon,\n",
        ")\n",
        "\n",
        "# Load the normal environment from gym\n",
        "environment_name = 'ALE/Breakout-v5'\n",
        "env = gym.make(environment_name, render_mode = \"rgb_array\")\n",
        "env = gym.wrappers.RecordEpisodeStatistics(env, deque_size=n_episodes)\n",
        "\n",
        "all_rewards = []\n",
        "\n",
        "for epoch in range(max_epochs):\n",
        "\n",
        "  # Actual Training code\n",
        "  for episode in tqdm(range(n_episodes)):\n",
        "      obs, info = env.reset()\n",
        "      state = get_states(obs)\n",
        "      done = False\n",
        "\n",
        "      # play one episode\n",
        "      while not done:\n",
        "          action = MAP_agent.get_action(state)\n",
        "          next_obs, reward, terminated, truncated, info = env.step(action)\n",
        "          next_state = get_states(next_obs)\n",
        "\n",
        "\n",
        "          # update the agent\n",
        "          MAP_agent.update(state, action, reward, terminated, next_state)\n",
        "\n",
        "          # update if the environment is done\n",
        "          done = terminated or truncated\n",
        "          state = next_state\n",
        "\n",
        "      all_rewards.append(info[\"episode\"][\"r\"][0])\n",
        "\n",
        "  # Saving rewards and q values\n",
        "  pickle.dump( MAP_agent.q_values, open( os.path.join(stem_path, 'MAP_qvalues.p'), \"wb\" ) )\n",
        "  pickle.dump( all_rewards, open( os.path.join(stem_path, 'MAP_rewards.p'), \"wb\" ) )\n",
        "\n",
        "  # decay the epsilon after each episode\n",
        "  MAP_agent.decay_epsilon()\n",
        "\n",
        "env.close()\n",
        "\n",
        "# Save trained q-values with pickle.dump"
      ],
      "metadata": {
        "id": "uV3x0oFptDUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stage 2 training (using epsilon decay for 100_000 episodes)"
      ],
      "metadata": {
        "id": "kgXd82n-tNSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load trained q-values from stage 1\n",
        "stem_path = '/content/agent_saves'\n",
        "trained_qvalues = pickle.load( open( os.path.join(stem_path, 'MAP_qvalues.p'), \"rb\" ) )\n",
        "\n",
        "today = date.today()\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.01\n",
        "n_episodes = 100_000\n",
        "start_epsilon = 0.6  # start with 60% exploration and 40% exploitation\n",
        "epsilon_decay = start_epsilon / (n_episodes / 2)  # reduce the exploration over time\n",
        "final_epsilon = 0.1\n",
        "\n",
        "# Q-Learning Agent\n",
        "MAP_agent = QLearningAgent(\n",
        "    learning_rate=learning_rate,\n",
        "    initial_epsilon=start_epsilon,\n",
        "    epsilon_decay=epsilon_decay,\n",
        "    final_epsilon=final_epsilon,\n",
        ")\n",
        "\n",
        "# Assign trained values to agent\n",
        "MAP_agent.q_values = trained_qvalues\n",
        "\n",
        "# Load the normal environment from gym\n",
        "environment_name = 'ALE/Breakout-v5'\n",
        "env = gym.make(environment_name, render_mode = \"rgb_array\")\n",
        "\n",
        "# Actual Training code\n",
        "env = gym.wrappers.RecordEpisodeStatistics(env, deque_size=n_episodes)\n",
        "all_rewards = []\n",
        "for episode in tqdm(range(n_episodes)):\n",
        "    obs, info = env.reset()\n",
        "    state = get_states(obs)\n",
        "    done = False\n",
        "\n",
        "    # play one episode\n",
        "    while not done:\n",
        "        action = MAP_agent.get_action(state)\n",
        "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
        "        next_state = get_states(next_obs)\n",
        "\n",
        "        # update the agent\n",
        "        MAP_agent.update(state, action, reward, terminated, next_state)\n",
        "\n",
        "        # update if the environment is done\n",
        "        done = terminated or truncated\n",
        "        state = next_state\n",
        "\n",
        "    # decay the epsilon after each episode\n",
        "    MAP_agent.decay_epsilon()\n",
        "\n",
        "    all_rewards.append(info[\"episode\"][\"r\"][0])\n",
        "\n",
        "    if episode % 20_000 == 0:\n",
        "        # Saving rewards and q values\n",
        "        pickle.dump( MAP_agent.q_values, open( os.path.join(stem_path, 'MAP_qvalues_2.p'), \"wb\" ) )\n",
        "        pickle.dump( all_rewards, open( os.path.join(stem_path, 'MAP_rewards_2.p'), \"wb\" ) )\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "id": "yCIbmC3StV3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## \"Training\" of random agent for 400_000 episodes"
      ],
      "metadata": {
        "id": "EPZwAVOeEyOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stem_path = '/content/agent_saves'\n",
        "\n",
        "# Rebuilding environment\n",
        "environment_name = 'ALE/Breakout-v5'\n",
        "env = gym.make(environment_name, render_mode = \"rgb_array\")\n",
        "n_games = 400_000\n",
        "env = gym.wrappers.RecordEpisodeStatistics(env, deque_size=n_games)\n",
        "random_train_rewards = []\n",
        "today = date.today()\n",
        "\n",
        "for game in tqdm(range(n_games)):\n",
        "  # Resetting environment variables\n",
        "  observation, info = env.reset()\n",
        "  done = False\n",
        "\n",
        "  # Random agent plays one game\n",
        "  while not done:\n",
        "    action = env.action_space.sample()\n",
        "    observation, reward, terminated, truncated, info = env.step(action)\n",
        "    done = terminated or truncated\n",
        "\n",
        "  random_train_rewards.append(info[\"episode\"][\"r\"][0])\n",
        "\n",
        "pickle.dump(random_train_rewards, open( os.path.join(stem_path, 'random_train_rewards.p'), \"wb\" ) )\n",
        "env.close()"
      ],
      "metadata": {
        "id": "UZmsPwofExYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load all_rewards"
      ],
      "metadata": {
        "id": "hGjjfx6rzAY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stem_path = '/content/agent_saves'\n",
        "stage1_rewards = pickle.load( open( os.path.join(stem_path, 'MAP_rewards_1.p'), \"rb\" ) )\n",
        "stage2_rewards = pickle.load( open( os.path.join(stem_path, 'MAP_rewards_2.p'), \"rb\" ) )\n",
        "combined_Q_rewards = stage1_rewards + stage2_rewards\n",
        "\n",
        "random_train_rewards = pickle.load( open( os.path.join(stem_path, 'random_train_rewards.p'), \"rb\" ) )"
      ],
      "metadata": {
        "id": "GOBSC5cPr2MG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Analysis of reward array"
      ],
      "metadata": {
        "id": "SC_afyTvW1q6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot rewards vs training episodes"
      ],
      "metadata": {
        "id": "eTsRZgHfrpEZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Choose the window size for the moving average\n",
        "window_size = 1000\n",
        "\n",
        "# Compute the moving average\n",
        "Q_moving_avg = np.convolve(combined_Q_rewards, np.ones(window_size)/window_size, mode='valid')\n",
        "rnd_moving_avg = np.convolve(random_train_rewards[:300_000], np.ones(window_size)/window_size, mode='valid')\n",
        "\n",
        "# Generate a list of epochs corresponding to the moving average\n",
        "epochs_avg = list(range(window_size, len(combined_Q_rewards) + 1))\n",
        "\n",
        "# Plotting moving average\n",
        "plt.plot(epochs_avg, Q_moving_avg, label='Our agent')\n",
        "plt.plot(epochs_avg, rnd_moving_avg, label='Random agent')\n",
        "\n",
        "# Set x-axis ticks at every 30,000 epochs\n",
        "plt.xticks(range(30000, len(combined_Q_rewards) + 1, 60000), minor = False)\n",
        "plt.xticks(range(30000, len(combined_Q_rewards) + 1, 30000), minor = True)\n",
        "\n",
        "# Add grid lines\n",
        "plt.grid(True, linestyle='--', alpha=0.7, which = 'major')\n",
        "plt.grid(True, linestyle='--', alpha=0.7, which = 'minor')\n",
        "\n",
        "# Set x-labels in scientific format\n",
        "plt.ticklabel_format(style='sci', axis='x', scilimits=(4,4))\n",
        "\n",
        "# Add title and labels\n",
        "plt.title('Epoch vs Average Rewards')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Avg Rewards')\n",
        "plt.legend()\n",
        "\n",
        "# Save the plot\n",
        "fig_stem_path = '/content/fig'\n",
        "today = date.today()\n",
        "plt.savefig(os.path.join(fig_stem_path, str(today) + '_epoch_vs_avg_rewards_plot.png'), bbox_inches='tight', dpi=300)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "scZipJMBrv5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q-learning agent performance evaluated over n_games"
      ],
      "metadata": {
        "id": "9aEDR8cGz_vE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load agent with trained Q-values\n",
        "stem_path = '/content/agent_saves'\n",
        "trained_qvalues = pickle.load( open( os.path.join(stem_path, 'MAP_qvalues_2.p'), \"rb\" ) )"
      ],
      "metadata": {
        "id": "xRV66K0x0leV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q-Learning Agent\n",
        "MAP_agent = QLearningAgent(\n",
        "    learning_rate=0.01,\n",
        "    initial_epsilon=1.0,\n",
        "    epsilon_decay=0.1,\n",
        "    final_epsilon=0.1,\n",
        ")\n",
        "MAP_agent.q_values = trained_qvalues\n",
        "\n",
        "performance_stem_path = '/content/performance_evaluation'\n",
        "\n",
        "# Rebuilding environment\n",
        "environment_name = 'ALE/Breakout-v5'\n",
        "env = gym.make(environment_name, render_mode = \"rgb_array\")\n",
        "n_games = 1000\n",
        "env = gym.wrappers.RecordEpisodeStatistics(env, deque_size=n_games)\n",
        "game_rewards = []\n",
        "today = date.today()\n",
        "\n",
        "for game in tqdm(range(n_games)):\n",
        "  # Resetting environment variables\n",
        "  observation, info = env.reset(seed=game)\n",
        "  state = get_states(observation)\n",
        "  done = False\n",
        "\n",
        "  # MAP agent plays one game\n",
        "  while not done:\n",
        "    action = MAP_agent.get_action(state)\n",
        "    observation, reward, terminated, truncated, info = env.step(action)\n",
        "    state = get_states(observation)\n",
        "    done = terminated or truncated\n",
        "  print(f'Game: {game}, Reward: {info[\"episode\"][\"r\"][0]}')\n",
        "  game_rewards.append(info[\"episode\"][\"r\"][0])\n",
        "\n",
        "pickle.dump(game_rewards, open( os.path.join(performance_stem_path, str(today) + 'Q_agent_rewards.p'), \"wb\" ) )\n",
        "env.close()"
      ],
      "metadata": {
        "id": "VGdLOTicz-5F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Random agent performance evaluation over n-games"
      ],
      "metadata": {
        "id": "xE9L_H441xSn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "performance_stem_path = '/content/performance_evaluation'\n",
        "\n",
        "# Rebuilding environment\n",
        "environment_name = 'ALE/Breakout-v5'\n",
        "env = gym.make(environment_name, render_mode = \"rgb_array\")\n",
        "n_games = 1000\n",
        "env = gym.wrappers.RecordEpisodeStatistics(env, deque_size=n_games)\n",
        "random_game_rewards = []\n",
        "today = date.today()\n",
        "\n",
        "for game in tqdm(range(n_games)):\n",
        "  # Resetting environment variables\n",
        "  observation, info = env.reset()\n",
        "  done = False\n",
        "\n",
        "  # Random agent plays one game\n",
        "  while not done:\n",
        "    action = env.action_space.sample()\n",
        "    observation, reward, terminated, truncated, info = env.step(action)\n",
        "    done = terminated or truncated\n",
        "\n",
        "  random_game_rewards.append(info[\"episode\"][\"r\"][0])\n",
        "\n",
        "pickle.dump(random_game_rewards, open( os.path.join(performance_stem_path, str(today) + 'random_train_rewards.p'), \"wb\" ) )\n",
        "env.close()"
      ],
      "metadata": {
        "id": "Fw2BAZS211d0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Count plots"
      ],
      "metadata": {
        "id": "O6vpzm5dvfqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "performance_stem_path = '/content/performance_evaluation'\n",
        "Q_agent_rewards = pickle.load( open( os.path.join(performance_stem_path, 'Q_agent_rewards.p'), \"rb\" ) )\n",
        "random_train_rewards = pickle.load( open( os.path.join(performance_stem_path, 'random_train_rewards.p'), \"rb\" ) )\n",
        "\n",
        "## Subplots\n",
        "n_games_df = pd.DataFrame({'Our agent': Q_agent_rewards, 'Random agent': random_train_rewards})\n",
        "int_df = n_games_df.astype(int)\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, sharey=True)\n",
        "fig.suptitle('Rewards Distribution over 1000 Games')\n",
        "\n",
        "# Plot the first histogram\n",
        "sns.countplot(x = 'Random agent', data=int_df, ax=ax1)\n",
        "ax1.bar_label(ax1.containers[0])\n",
        "ax1.set_ylabel('Frequency')\n",
        "\n",
        "# Plot the second histogram\n",
        "sns.countplot(x = 'Our agent',data=int_df, ax=ax2)\n",
        "ax2.bar_label(ax2.containers[0])\n",
        "ax2.set_ylabel('')\n",
        "\n",
        "# Save the plot\n",
        "fig_stem_path = '/content/fig'\n",
        "today = date.today()\n",
        "plt.savefig(os.path.join(fig_stem_path, str(today) + '_countplots.png'), bbox_inches='tight', dpi=300)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "biY3kadrpPDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Make agent play game and record video."
      ],
      "metadata": {
        "id": "rXsDJEngsNgf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stem_path = '/content/agent_saves'\n",
        "trained_qvalues = pickle.load( open( os.path.join(stem_path, 'MAP_qvalues_2.p'), \"rb\" ) )"
      ],
      "metadata": {
        "id": "L83h3aSb5J85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q-Learning Agent\n",
        "MAP_agent = QLearningAgent(\n",
        "    learning_rate=0.01,\n",
        "    initial_epsilon=0.0,\n",
        "    epsilon_decay=0.1,\n",
        "    final_epsilon=0.1,\n",
        ")\n",
        "\n",
        "MAP_agent.q_values = trained_qvalues\n",
        "\n",
        "# Rebuilding environment\n",
        "environment_name = 'ALE/Breakout-v5'\n",
        "env = gym.make(environment_name, render_mode = \"rgb_array\")\n",
        "\n",
        "today = date.today()\n",
        "\n",
        "# Setting up video recording\n",
        "env = gym.wrappers.RecordVideo(env, '/content/videos', name_prefix = 'MAP_video')\n",
        "\n",
        "# Resetting environment variables\n",
        "observation, info = env.reset()\n",
        "state = get_states(observation)\n",
        "done = False\n",
        "\n",
        "# MAP agent plays one game\n",
        "while not done:\n",
        "  action = MAP_agent.get_action(state)\n",
        "  observation, reward, terminated, truncated, info = env.step(action)\n",
        "  state = get_states(observation)\n",
        "  done = terminated or truncated\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "id": "HandhTInsA9t"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}